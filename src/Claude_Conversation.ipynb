{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>Interaction with Claude API</h1>\n",
   "id": "74d6092282867be2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook introduced Claud API capabilities.<br>\n",
    "This notebook started from codes in <a href=\"https://anthropic.skilljar.com/claude-with-the-anthropic-api/\" target=\"_blank\">Anthrophic Building with the Claude API</a> course.\n",
    "<br><br>\n",
    "Demonstrates\n",
    "- How context influences generated completion\n",
    "- How **system prompt** influences generated completion\n",
    "- How **assistant prompt** influences generated completion\n",
    "- How **temperature** (low,high) influences generated completion\n",
    "- How generated completions consumed via **streaming** interface\n",
    "- How **stop_sequences** stops generation in addition to **max_tokens**"
   ],
   "id": "b119a4e2179f9fc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Setup</h2>\n",
    "Install anthropic and python-dotenv libraries<br>\n",
    "python-dotenv allows program to load configuration data from .env file. <br>"
   ],
   "id": "4053b7d9e39b7185"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install anthropic python-dotenv",
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "AntrophicPrompt class keeps track of conversation history to support the context. <br>\n",
    "Class accumulates user and assistant prompts.<br>\n",
    "There is only one system prompt. <br>\n",
    "Method clear_history() clears all accumulated prompts. <br>\n",
    "Default model is \"claude-3-5-haiku-latest\"<br>"
   ],
   "id": "e5b855e1588e3012"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T02:35:38.442410Z",
     "start_time": "2025-09-23T02:35:37.427439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "class AntrophicPrompt():\n",
    "\n",
    "    def __init__(self, model=\"claude-3-5-haiku-latest\"):\n",
    "        self.model = model\n",
    "        self.client = Anthropic()\n",
    "        self.messages = []\n",
    "        self.user_message = None\n",
    "        self.assistant_message = None\n",
    "        self.system_message = None\n",
    "\n",
    "    def add_user_message(self, text):\n",
    "        self.user_message = {\"role\": \"user\", \"content\": text}\n",
    "        self.messages.append(self.user_message)\n",
    "\n",
    "\n",
    "    def add_assistant_message(self, text):\n",
    "        self.assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "        self.messages.append(self.assistant_message)\n",
    "\n",
    "\n",
    "    def set_system_message(self, text):\n",
    "        self.system_message = text\n",
    "\n",
    "    def gen_completion(self, system=None, temperature=1.0, stop_sequences=[]):\n",
    "        params = {\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": self.messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop_sequences\": stop_sequences,\n",
    "        }\n",
    "\n",
    "        if system:\n",
    "            params[\"system\"] = system\n",
    "        elif self.system_message:\n",
    "            params[\"system\"] = self.system_message\n",
    "\n",
    "        message = self.client.messages.create(**params)\n",
    "        return message.content[0].text\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.messages = []\n",
    "        self.user_message = None\n",
    "        self.assistant_message = None\n",
    "        self.system_message = None\n",
    "        self.model = \"claude-3-5-haiku-latest\""
   ],
   "id": "8fa48b9f4eed57f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get API key (from .env file)",
   "id": "18e121d5ff1b772b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T02:35:43.081643Z",
     "start_time": "2025-09-23T02:35:43.065840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load API Key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "3761ce9e83c11b56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's test by generating the first completion.",
   "id": "449454b0d926abd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T02:35:48.192279Z",
     "start_time": "2025-09-23T02:35:46.118002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "model = \"claude-sonnet-4-0\"\n",
    "client = AntrophicPrompt(model)\n",
    "\n",
    "user_prompt = \"Define quantum computing in one sentence\"\n",
    "client.add_user_message(user_prompt)\n",
    "\n",
    "#Get Completion\n",
    "answer = client.gen_completion()\n",
    "print(answer)"
   ],
   "id": "f004b77d081c8a01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a type of computation that harnesses quantum mechanical phenomena like superposition and entanglement to process information in ways that can potentially solve certain problems exponentially faster than classical computers.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following example demonstrates how the **context** helps. <br>\n",
    "The user prompt is \"Write another sentence\" and this prompt without a context is very ambiguous. <br>\n",
    "Adding the prior answer as \"assistant prompt\" established a context to generate for the given \"user prompt\". <br>"
   ],
   "id": "6a821111476e88af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T01:39:31.124534Z",
     "start_time": "2025-09-23T01:39:27.884456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.add_assistant_message(answer)\n",
    "user_prompt = \"Write another sentence\"\n",
    "client.add_user_message(user_prompt)\n",
    "answer = client.gen_completion()\n",
    "print(answer)"
   ],
   "id": "5fc0c798738ccd67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computers use quantum bits (qubits) that can exist in multiple states simultaneously, unlike classical bits that are limited to just 0 or 1, enabling them to perform many calculations in parallel.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following example demonstrates how to implement conversation via Claude API <br>\n",
    "- What is 2+3?\n",
    "- Add 7 to that\n",
    "- Divide by 2 and if the result is even print \"even\" otherwise print \"odd\""
   ],
   "id": "3da23b279f7c7618"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T01:50:27.764880Z",
     "start_time": "2025-09-23T01:49:37.128014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.clear_history()\n",
    "i=0\n",
    "while i < 3:\n",
    "    user_input=input(\"?>\")\n",
    "    print(\"user_input: \"+user_input)\n",
    "    client.add_user_message(user_input)\n",
    "    answer = client.gen_completion()\n",
    "    client.add_assistant_message(answer)\n",
    "    print(\"------------\")\n",
    "    print(answer)\n",
    "    print(\"------------\")\n",
    "    i += 1"
   ],
   "id": "7da648dd448b00e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input: What is 2+5?\n",
      "------------\n",
      "7\n",
      "------------\n",
      "user_input: Add 7 to that\n",
      "------------\n",
      "7 + 7 = 14\n",
      "------------\n",
      "user_input: Divide by 2 and if the result is even print \"even\" otherwise print \"odd\"\n",
      "------------\n",
      "Let me solve this step by step:\n",
      "\n",
      "1. 14 ÷ 2 = 7\n",
      "2. 7 is an odd number\n",
      "\n",
      "So, the result is \"odd\"\n",
      "------------\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>How does system prompt influence the generated completion?</h2>\n",
    "The effect of \"system prompt\" when generating a completion.<br>\n",
    "Here it is used to assign a role to LLM <br>"
   ],
   "id": "7ae0235533f914af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T02:15:07.982920Z",
     "start_time": "2025-09-23T02:14:59.038780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start a new conversation\n",
    "client.clear_history()\n",
    "user_prompt = \"How to solve x^2+2x=-1 for x?\"\n",
    "client.add_user_message(user_prompt)\n",
    "\n",
    "# Without system prompt\n",
    "answer = client.gen_completion()\n",
    "print(f\"Answer without system prompt : \"+answer)\n",
    "\n",
    "# With system prompt\n",
    "system = \"\"\"\n",
    "You are a patient math tutor.\n",
    "Do not directly answer a student's questions.\n",
    "Guide them to a solution step by step.\n",
    "\"\"\"\n",
    "client.set_system_message(system)\n",
    "answer2 = client.gen_completion()\n",
    "print(f\"Answer with system prompt : \"+answer2)"
   ],
   "id": "45311865c5052616",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer without system prompt : Let's solve this step by step:\n",
      "\n",
      "1) First, rearrange the equation to standard quadratic form by moving all terms to one side:\n",
      "   x^2 + 2x + 1 = 0\n",
      "\n",
      "2) This is a quadratic equation in the form ax^2 + bx + c = 0\n",
      "   Where a = 1, b = 2, and c = 1\n",
      "\n",
      "3) We can solve this using the quadratic formula: x = [-b ± √(b^2 - 4ac)] / (2a)\n",
      "\n",
      "4) Substitute the values:\n",
      "   x = [-2 ± √(2^2 - 4(1)(1))] / (2(1))\n",
      "\n",
      "5) Simplify under the square root:\n",
      "   x = [-2 ± √(4 - 4)] / 2\n",
      "   x = [-2 ± √0] / 2\n",
      "   x = [-2 ± 0] / 2\n",
      "\n",
      "6) This gives two solutions:\n",
      "   x = -2/2 = -1\n",
      "\n",
      "Therefore, the solution is x = -1.\n",
      "\n",
      "You can verify this by plugging -1 back into the original equation:\n",
      "(-1)^2 + 2(-1) = 1 - 2 = -1 ✓\n",
      "Answer with system prompt : Let's solve this step by step:\n",
      "\n",
      "1. First, let's rearrange the equation to standard quadratic form\n",
      "   • Move all terms to one side of the equation\n",
      "   • Add 1 to both sides\n",
      "   x^2 + 2x + 1 = 0\n",
      "\n",
      "2. This looks like a perfect square trinomial. Can you recognize the pattern?\n",
      "   • Do you see how x^2 + 2x + 1 can be factored?\n",
      "\n",
      "3. Hint: What number, when squared, gives 1? \n",
      "   • This suggests we can factor it as (x + 1)^2\n",
      "\n",
      "4. Would you like to try factoring the left side of the equation?\n",
      "\n",
      "5. After factoring, what would the next step be to solve for x?\n",
      "\n",
      "Would you like to try working through these steps?\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first test, LLM gives the answer and shows how it solved step by step. <br>\n",
    "But the second answer is adjusted based on LLM is playing \"a patient math tutor\" and guides the user via questions. <br>"
   ],
   "id": "a85eb319e842f05c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>How does temperature influences generated completion?</h2>\n",
   "id": "28f029218553dfa9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T02:40:46.049701Z",
     "start_time": "2025-09-23T02:40:42.432519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start a new conversation\n",
    "client.clear_history()\n",
    "user_prompt = \"Generate a one sentence unique book idea.\"\n",
    "client.add_user_message(user_prompt)\n",
    "\n",
    "# Keep temperature at the lower value\n",
    "i=0\n",
    "temperature=0.1\n",
    "while i < 3:\n",
    "    answer = client.gen_completion(temperature=temperature)\n",
    "    print(f\"Answer[{i}] with T={temperature} : \\n{answer}\\n\")\n",
    "    i += 1"
   ],
   "id": "87a0b242622fd54b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer[0] with T=0.1 : \n",
      "A reclusive botanist discovers a rare plant that can communicate telepathically, revealing ancient secrets about humanity's forgotten connection to the natural world.\n",
      "\n",
      "Answer[1] with T=0.1 : \n",
      "A reclusive botanist discovers a rare plant that can communicate telepathically, revealing ancient secrets about humanity's forgotten connection to the natural world.\n",
      "\n",
      "Answer[2] with T=0.1 : \n",
      "A reclusive botanist discovers a rare plant that can communicate telepathically, revealing ancient secrets about humanity's forgotten connection to the natural world.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Due to low temperature, all generations look similar.",
   "id": "2f0cc75ffb82e36e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T02:41:10.751706Z",
     "start_time": "2025-09-23T02:41:06.126631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Increase Temperature tp 0.95 for diverse completions\n",
    "i=0\n",
    "temperature=1.0\n",
    "while i < 3:\n",
    "    answer = client.gen_completion(temperature=temperature)\n",
    "    print(f\"Answer[{i}] with T={temperature} : \\n{answer}\\n\")\n",
    "    i += 1"
   ],
   "id": "81f6230af4f9ba59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer[0] with T=1.0 : \n",
      "A disgraced quantum physicist discovers she can communicate with her alternate selves across parallel universes, each living a radically different life, and must collaborate with these versions of herself to prevent a catastrophic interdimensional collapse.\n",
      "\n",
      "Answer[1] with T=1.0 : \n",
      "A former cryptographer discovers an ancient encryption method that allows her to communicate with parallel versions of herself across different timelines, uncovering a cosmic conspiracy that threatens the fabric of reality.\n",
      "\n",
      "Answer[2] with T=1.0 : \n",
      "A skilled botanist discovers a rare plant that can communicate telepathically, revealing ancient secrets about humanity's forgotten connection to the natural world.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first two ideas are different but the last book idea is the same as the first set of book ideas generated.",
   "id": "ee7387aa5002bead"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ce2210d9fe0f25e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T02:59:03.906609Z",
     "start_time": "2025-09-23T02:58:59.853390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.clear_history()\n",
    "user_prompt = \"Generate database query to show what percentage of daily active users were using a given feature. Ensure that output is in SQL.\"\n",
    "client.add_user_message(user_prompt)\n",
    "client.add_assistant_message(\"```SQL\")\n",
    "\n",
    "# stop sequences, to stop generation\n",
    "stop_sequences=[\"```\"]\n",
    "text = client.gen_completion(stop_sequences=stop_sequences)\n",
    "text"
   ],
   "id": "8c6d25f4dc3f936",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWITH daily_active_users AS (\\n    SELECT \\n        DATE(login_timestamp) AS active_date,\\n        COUNT(DISTINCT user_id) AS total_daily_active_users\\n    FROM user_logins\\n    GROUP BY DATE(login_timestamp)\\n),\\n\\nfeature_users AS (\\n    SELECT \\n        DATE(feature_usage_timestamp) AS feature_date,\\n        COUNT(DISTINCT user_id) AS feature_active_users\\n    FROM feature_usage_log\\n    WHERE feature_name = 'target_feature'\\n    GROUP BY DATE(feature_usage_timestamp)\\n)\\n\\nSELECT \\n    f.feature_date,\\n    f.feature_active_users,\\n    d.total_daily_active_users,\\n    ROUND(\\n        (f.feature_active_users * 100.0) / d.total_daily_active_users, \\n        2\\n    ) AS feature_usage_percentage\\nFROM feature_users f\\nJOIN daily_active_users d ON f.feature_date = d.active_date\\nORDER BY f.feature_date;\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T03:00:00.817121Z",
     "start_time": "2025-09-23T03:00:00.811619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(text)"
   ],
   "id": "528ddf1ed6b5f5be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\nWITH daily_active_users AS (\n    SELECT \n        DATE(login_timestamp) AS active_date,\n        COUNT(DISTINCT user_id) AS total_daily_active_users\n    FROM user_logins\n    GROUP BY DATE(login_timestamp)\n),\n\nfeature_users AS (\n    SELECT \n        DATE(feature_usage_timestamp) AS feature_date,\n        COUNT(DISTINCT user_id) AS feature_active_users\n    FROM feature_usage_log\n    WHERE feature_name = 'target_feature'\n    GROUP BY DATE(feature_usage_timestamp)\n)\n\nSELECT \n    f.feature_date,\n    f.feature_active_users,\n    d.total_daily_active_users,\n    ROUND(\n        (f.feature_active_users * 100.0) / d.total_daily_active_users, \n        2\n    ) AS feature_usage_percentage\nFROM feature_users f\nJOIN daily_active_users d ON f.feature_date = d.active_date\nORDER BY f.feature_date;\n"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DAU: Calculates Daily Active Users from user_logins table by counting the unique number of user logins in a given date<br>\n",
    "DFU: Calculates Daily Feature Usage from feature_user_log table by counting the unique number of user identifiers in a given date<br>\n",
    "feature_usage_percentage: calculated by using daily numbers in DAU and DFU <br>"
   ],
   "id": "d91f6fe5e2155c59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "41be5622b4e72d34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "1. [Handling Stop Reasons](https://docs.claude.com/en/api/handling-stop-reasons)"
   ],
   "id": "8375d1ac9c64bcb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tim-test-env]",
   "language": "python",
   "name": "conda-env-tim-test-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
